{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe43590-57af-42d1-8884-bcdaa2426286",
   "metadata": {},
   "source": [
    "# Model Development - Ryanair Customer Query Classification \n",
    "## **Objective**: Build and compare multiple machine learning models for customer query classification. \n",
    "## **Model Hierarchy Strategy:**\n",
    "### 1. **Baseline**: TF-IDF + Logistic Regression (Fast & Simple)\n",
    "### 2. **Advanced**: TF-IDF + SVM (More Powerful)\n",
    "### 3. **Traditional ML**: Random Forest + Naive Bayes (Robust Alternatives)\n",
    "### 4. **Gradient Boosting**: XGBoost + LightGBM + CatBoost (State-of-the-art)\n",
    "### 5. **Ensemble**: Voting Classifier (Best Performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44b055c-4e76-4d6e-9c8e-b72e5b829359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import  libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, \n",
    "    precision_recall_fscore_support, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import joblib\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c73dac9-06e8-47bb-ad9d-7205eafb9e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Processed Data...\n",
      "Data loaded successfully!\n",
      "   Training: 16,000 samples\n",
      "   Validation: 4,000 samples\n",
      "   Test: 5,976 samples\n",
      "   Classes: 30\n",
      "\n",
      "Data shapes:\n",
      "   X_train: (16000,)\n",
      "   y_train: (16000,)\n",
      "   X_val: (4000,)\n",
      "   y_val: (4000,)\n"
     ]
    }
   ],
   "source": [
    "# Load processed data\n",
    "print(\"Loading Processed Data...\")\n",
    "# Load training splits\n",
    "train_split = pd.read_csv('../data/processed/train_split.csv')\n",
    "val_split = pd.read_csv('../data/processed/val_split.csv')\n",
    "test_processed = pd.read_csv('../data/processed/test_processed.csv')\n",
    "\n",
    "# Load label encoder\n",
    "with open('../data/processed/label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Load preprocessing info\n",
    "with open('../data/processed/preprocessing_info.json', 'r') as f:\n",
    "    preprocessing_info = json.load(f)\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"   Training: {len(train_split):,} samples\")\n",
    "print(f\"   Validation: {len(val_split):,} samples\") \n",
    "print(f\"   Test: {len(test_processed):,} samples\")\n",
    "print(f\"   Classes: {preprocessing_info['n_classes']}\")\n",
    "\n",
    "# Prepare data\n",
    "X_train = train_split['query_original']\n",
    "y_train = train_split['label_encoded']\n",
    "X_val = val_split['query_original']\n",
    "y_val = val_split['label_encoded']\n",
    "X_test = test_processed['query_lemmatized']  # For final predictions\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   y_train: {y_train.shape}\")\n",
    "print(f\"   X_val: {X_val.shape}\")\n",
    "print(f\"   y_val: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122c862-9446-4b39-9775-6f2a508b7024",
   "metadata": {},
   "source": [
    "## 1. Baseline Model: TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21a30c1-feaa-47cf-b0e5-e40f9acc0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTracker:\n",
    "    \"\"\"Track model performance and artifacts.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.training_times = {}\n",
    "        \n",
    "    def add_model(self, name, model, X_train, y_train, X_val, y_val, vectorizer=None):\n",
    "        \"\"\"Train and evaluate a model.\"\"\"\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train model\n",
    "        if vectorizer:\n",
    "            # For traditional ML models with vectorization\n",
    "            X_train_vec = vectorizer.fit_transform(X_train)\n",
    "            X_val_vec = vectorizer.transform(X_val)\n",
    "            model.fit(X_train_vec, y_train)\n",
    "            y_pred = model.predict(X_val_vec)\n",
    "            y_pred_proba = model.predict_proba(X_val_vec) if hasattr(model, 'predict_proba') else None\n",
    "        else:\n",
    "            # For models that handle raw text\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_pred_proba = model.predict_proba(X_val) if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        training_time = time.time() - start_time\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='weighted')\n",
    "        \n",
    "        # Store results\n",
    "        self.models[name] = {\n",
    "            'model': model,\n",
    "            'vectorizer': vectorizer,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        self.results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'training_time': training_time\n",
    "        }\n",
    "        \n",
    "        self.training_times[name] = training_time\n",
    "        \n",
    "        print(f\"{name} completed!\")\n",
    "        print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"   F1-Score: {f1:.4f}\")\n",
    "        print(f\"   Training Time: {training_time:.2f}s\")\n",
    "        \n",
    "        return self.results[name]\n",
    "    \n",
    "    def get_results_df(self):\n",
    "        \"\"\"Get results as DataFrame.\"\"\"\n",
    "        return pd.DataFrame(self.results).T\n",
    "\n",
    "# Initialize model tracker\n",
    "tracker = ModelTracker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc23f222-671b-4757-a5bb-632c7f8317d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Data Quality Check Before Training:\n",
      "X_train shape: (16000,)\n",
      "X_val shape: (4000,)\n",
      "X_train missing values: 1\n",
      "X_val missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# VERİ KONTROLÜ VE TEMİZLEME\n",
    "print(\"🔍 Data Quality Check Before Training:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "\n",
    "# Missing values kontrolü\n",
    "print(f\"X_train missing values: {X_train.isna().sum()}\")\n",
    "print(f\"X_val missing values: {X_val.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca109768-83e4-4891-bad6-4e223f5ff2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. BASELINE MODEL: TF-IDF + Logistic Regression\n",
      "============================================================\n",
      "\n",
      "Training Baseline_LR...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     15\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m LogisticRegression(\n\u001b[0;32m     16\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     17\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m     18\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Handle class imbalance\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m baseline_results \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBaseline_LR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_vectorizer\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mModelTracker.add_model\u001b[1;34m(self, name, model, X_train, y_train, X_val, y_val, vectorizer)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vectorizer:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# For traditional ML models with vectorization\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     X_train_vec \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     X_val_vec \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_val)\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train_vec, y_train)\n",
      "File \u001b[1;32m~\\Documents\\Desktop\\ml-customer-support-classifier\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2103\u001b[0m )\n\u001b[1;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\Desktop\\ml-customer-support-classifier\\venv\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Documents\\Desktop\\ml-customer-support-classifier\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\Desktop\\ml-customer-support-classifier\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1262\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1264\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1265\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\Documents\\Desktop\\ml-customer-support-classifier\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:99\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    101\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32m~\\Documents\\Desktop\\ml-customer-support-classifier\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:232\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    229\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# 1. BASELINE: TF-IDF + Logistic Regression\n",
    "print(\"1. BASELINE MODEL: TF-IDF + Logistic Regression\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    min_df=2,            # Ignore terms in less than 2 documents\n",
    "    max_df=0.95,         # Ignore terms in more than 95% of documents\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "baseline_results = tracker.add_model(\n",
    "    'Baseline_LR', lr_model, X_train, y_train, X_val, y_val, tfidf_vectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3829aa2b-05c4-4447-8109-c2690fd932ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
